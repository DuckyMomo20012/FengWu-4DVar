{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-ZXQLd7AQh8"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "\n",
        "!pip install zarr\n",
        "!pip install xarray\n",
        "!pip install xarray[complete]\n",
        "!pip install minio\n",
        "!pip install nest-asyncio\n",
        "import zarr\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import minio\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcUQddrPDHP3"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='planar-osprey-377213')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSkvocgz47av"
      },
      "outputs": [],
      "source": [
        "AWS_S3_ENDPOINT_URL = f'3884f8fb961f5917cb4c1d60789aad89.loophole.site'\n",
        "\n",
        "s3 = minio.Minio(\n",
        "  AWS_S3_ENDPOINT_URL,\n",
        "  \"GKfc29de6c063e01cf88535057\",\n",
        "  \"05f6389438696ef6d5debfbf7d6dac96e4102a0386a9856a93d783de4feb8a51\",\n",
        "  # Force the region, this is specific to garage\n",
        "  region=\"garage\",\n",
        ")\n",
        "\n",
        "response = s3.list_buckets()\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKoKAf9MH-j"
      },
      "outputs": [],
      "source": [
        "# Open the Zarr dataset\n",
        "# ds = xr.open_zarr(\"gs://gcp-public-data-arco-era5/ar/1959-2022-6h-1440x721.zarr\")\n",
        "ds = xr.open_zarr(\"gs://gcp-public-data-arco-era5/ar/1959-2022-full_37-6h-0p25deg_derived.zarr\")\n",
        "\n",
        "# Define filtering parameters\n",
        "start_year = \"2019-01-01\"\n",
        "end_year = \"2019-01-10\"\n",
        "selected_levels = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
        "\n",
        "# Filter dataset\n",
        "resampled_ds = ds.sel(\n",
        "    time=slice(start_year, end_year),  # Filter by time range\n",
        "    level=selected_levels  # Filter by pressure levels\n",
        ")\n",
        "\n",
        "resampled_ds = resampled_ds.where((resampled_ds.time.dt.hour == 0) | (resampled_ds.time.dt.hour == 6), drop=True)\n",
        "\n",
        "# Resample to 128x256 spatial resolution\n",
        "# resampled_ds = resampled_ds.interp(\n",
        "#     latitude=pd.Series(np.linspace(resampled_ds.latitude.min(), resampled_ds.latitude.max(), 128)),\n",
        "#     longitude=pd.Series(np.linspace(resampled_ds.longitude.min(), resampled_ds.longitude.max(), 256)),\n",
        "#     method='nearest'\n",
        "# )\n",
        "\n",
        "# Ensure that specific variables are included\n",
        "resampled_ds = resampled_ds[['10m_u_component_of_wind', '10m_v_component_of_wind',\n",
        "                            '2m_temperature', 'mean_sea_level_pressure',\n",
        "                            'geopotential', 'specific_humidity',\n",
        "                            'u_component_of_wind', 'v_component_of_wind',\n",
        "                            'temperature']]\n",
        "resampled_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNzyPLSnc8xU"
      },
      "outputs": [],
      "source": [
        "def uploadFileToS3(bucket_name, object_name, data):\n",
        "\n",
        "    npy_buffer = io.BytesIO()\n",
        "    np.save(npy_buffer, data)\n",
        "    npy_buffer.seek(0)  # Move to the start of the buffer\n",
        "\n",
        "    s3.put_object(\n",
        "        bucket_name=bucket_name,\n",
        "        object_name=object_name,\n",
        "        data=npy_buffer,\n",
        "        length=npy_buffer.getbuffer().nbytes,\n",
        "        content_type=\"application/octet-stream\"\n",
        "    )\n",
        "\n",
        "# Test upload\n",
        "# uploadFile(\"era-bucket\", \"tmp.npy\", np.array([1, 2, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-eca9k-Ncx9"
      },
      "outputs": [],
      "source": [
        "single_level_vnames = [\"u10\", \"v10\", \"t2m\", \"msl\"]\n",
        "multi_level_vnames = [\"z\", \"q\", \"u\", \"v\", \"t\"]\n",
        "height_level = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
        "\n",
        "single_level_mapping = {\n",
        "    \"10m_u_component_of_wind\": \"u10\",\n",
        "    \"10m_v_component_of_wind\": \"v10\",\n",
        "    \"2m_temperature\": \"t2m\",\n",
        "    \"mean_sea_level_pressure\": \"msl\",\n",
        "}\n",
        "\n",
        "multi_level_mapping = {\n",
        "    \"geopotential\": \"z\",\n",
        "    \"specific_humidity\": \"q\",\n",
        "    \"u_component_of_wind\": \"u\",\n",
        "    \"v_component_of_wind\": \"v\",\n",
        "    \"temperature\": \"t\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "async def uploadMultiLevel(time):\n",
        "    time_value = str(resampled_ds.isel(time=time).time.values)\n",
        "    timestamp = str(time_value).split(\".\")[0]\n",
        "    year = pd.Timestamp(time_value).year\n",
        "\n",
        "    async def uploadKey(vname):\n",
        "        file = os.path.join(str(year), timestamp).replace(\"T\", \"/\")\n",
        "\n",
        "        async def uploadLevel(levelIdx):\n",
        "            height = height_level[levelIdx]\n",
        "            data = resampled_ds.isel(time=time)[vname][levelIdx]\n",
        "\n",
        "            mapped_vname = multi_level_mapping[vname]\n",
        "\n",
        "            url = f\"{file}-{mapped_vname}-{height}.0.npy\"\n",
        "\n",
        "            uploadFileToS3(\"era-bucket\", url, data.values)\n",
        "            print(f\"Upload file {url}\")\n",
        "\n",
        "\n",
        "        await asyncio.gather(*[uploadLevel(i) for i in range(13)])\n",
        "\n",
        "    await asyncio.gather(*[uploadKey(vname) for vname in multi_level_mapping.keys()])\n",
        "\n",
        "\n",
        "async def uploadSingleLevel(time):\n",
        "    time_value = str(resampled_ds.isel(time=time).time.values)\n",
        "    timestamp = str(time_value).split(\".\")[0]\n",
        "    year = pd.Timestamp(time_value).year\n",
        "\n",
        "    async def uploadKey(vname):\n",
        "        data = resampled_ds.isel(time=time)[vname]\n",
        "\n",
        "        mapped_vname = single_level_mapping[vname]\n",
        "\n",
        "        file = os.path.join(\"single/\", str(year), timestamp).replace(\"T\", \"/\")\n",
        "        url = f\"{file}-{mapped_vname}.npy\"\n",
        "\n",
        "        uploadFileToS3(\"era-bucket\", url, data.values)\n",
        "        print(f\"Upload file {url}\")\n",
        "\n",
        "    await asyncio.gather(*[uploadKey(vname) for vname in single_level_mapping.keys()])\n",
        "\n",
        "async def uploadAll(time):\n",
        "    await asyncio.gather(\n",
        "        uploadMultiLevel(time),\n",
        "        uploadSingleLevel(time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqAsQcqi2Kda"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    # Run all uploads concurrently\n",
        "    await asyncio.gather(*[uploadAll(i) for i in range(len(resampled_ds[\"time\"]))])\n",
        "\n",
        "asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}